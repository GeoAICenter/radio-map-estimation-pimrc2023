{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the button below to open this notebook in Google Colab. Note that changes made to the notebook in Colab will not be reflected in Github, nor can the notebook be saved on Colab without first making a copy. \n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nikitalokhmachev-ai/radio-map-estimation-public/blob/main/notebooks/Visualize_Inputs_Outputs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If opened in Colab, set `using_colab` to `True` in the code block below, then run the second and (optionally) third blocks. The second block will install the needed version of joblib for the data scaler to be loaded, then clone the github repository into Colab's local storage in order to load the models and other functions. The third block will connect to Google Drive (user login required), which allows the Colab notebook to read and write data to the drive (e.g. training data or evaluation results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using_colab = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    %cd /content/\n",
        "    !rm -rf /content/radio-map-estimation-public\n",
        "    !git clone https://github.com/nikitalokhmachev-ai/radio-map-estimation-public.git\n",
        "    !pip install -q -r /content/radio-map-estimation-public/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Untar Validation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We visualize the inputs and outputs of the validation data, but you can use any data you choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jdtnv8-45Kd"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!tar -xkf '/Path/to/saved/tar/file' -C '/path/to/save/untarred/files'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWfrHtpz0pbf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import joblib\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import model architectures and data structures\n",
        "\n",
        "os.chdir('path/to/repository')\n",
        "from data_utils import MapDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed, define device\n",
        "\n",
        "seed = 3\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set batch size\n",
        "val_batch_size = 1024\n",
        "\n",
        "# Manually set values for buildings, unsampled locations, and sampled locations in the environment mask. \n",
        "# For the models in the PIMRC paper, these are set to \"None\", meaning they keep the default values of -1, 0, and 1 respectively.\n",
        "building_value = None\n",
        "unsampled_value = None\n",
        "sampled_value = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify folder containing trained models\n",
        "model_folder = '/folder/with/trained/models'\n",
        "\n",
        "# Specify path to untarred validation data\n",
        "val_data_folder = '/path/to/untarred/validation/data'\n",
        "\n",
        "# Specify path to data scaler\n",
        "scaler_path = 'scalers/minmax_scaler_zero_min134.joblib'\n",
        "\n",
        "# Set folder to save visualizations\n",
        "viz_folder = '/Path/to/save/visualizations'\n",
        "if not os.path.exists(viz_folder):\n",
        "    os.makedirs(viz_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntaz71nG4tBi"
      },
      "source": [
        "# Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_HoJ4XRehm4"
      },
      "outputs": [],
      "source": [
        "def get_model_output(x, channel_id, model, model_layer):\n",
        "  #x: bs, c, h, w\n",
        "  x = x.to(device)\n",
        "  activation = {}\n",
        "  def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "  model_layer.register_forward_hook(get_activation('out'))\n",
        "  output = model(x)\n",
        "\n",
        "  return activation['out'][0].permute(1,2,0).detach().cpu()[:,:,channel_id].unsqueeze(-1).numpy()\n",
        "\n",
        "def visualize_channels(x, model, model_layer, nchannels, nrows, ncols, figsize=(15, 15)):\n",
        "  fig, axs = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "  for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "      channel_id = i * nrows + j\n",
        "      axs[i, j].imshow(get_model_output(x, channel_id, model, model_layer))\n",
        "      axs[i, j].set_title(str(channel_id))\n",
        "      channel_id += 1\n",
        "      if channel_id >= nchannels:\n",
        "        break\n",
        "\n",
        "def visualize_channel(x, channel_id, model, model_layer):\n",
        "  plt.imshow(get_model_output(x, channel_id, model, model_layer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Validation data into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKlRHRvlGuXb"
      },
      "outputs": [],
      "source": [
        "val_pickle_path = os.path.join(val_data_folder, '*.pickle')\n",
        "val_pickles = glob.glob(val_pickle_path)\n",
        "\n",
        "with open(scaler_path, 'rb') as f:\n",
        "  scaler = joblib.load(f)\n",
        "\n",
        "val_ds = MapDataset(val_pickles, scaler=scaler)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Input and Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Sample Map and Environment Mask are used as inputs to the model. The complete Radio Map is the ground truth that the model seeks to recreate. We use the example map shown in the paper below, but this can be replaced with any image from the validation set or other dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAtgjS37EtE-"
      },
      "outputs": [],
      "source": [
        "# Example map\n",
        "example_map = os.path.join(val_data_folder, 'test_0.01%_batch_0.pickle')\n",
        "map_idx=37\n",
        "\n",
        "# Load\n",
        "t_x_points, t_channel_pows, t_y_masks = np.load(example_map, allow_pickle=True)\n",
        "\n",
        "# Manually preprocess map (this would normally be done by the MapDataset class)\n",
        "t_y_points = t_channel_pows * t_y_masks\n",
        "t_x_masks = t_x_points[:,1,:,:] == 1\n",
        "t_x_points[:,0,:,:] = scaler.transform(t_x_points[:,0,:,:]) * t_x_masks\n",
        "t_channel_pows = scaler.transform(t_channel_pows)\n",
        "t_y_points = scaler.transform(t_y_points)\n",
        "\n",
        "sample_map = t_x_points[map_idx,0,:,:]\n",
        "env_mask = t_x_points[map_idx,1,:,:]\n",
        "target = t_y_points[map_idx,0,:,:]\n",
        "target[env_mask==-1] = 1\n",
        "\n",
        "# Visualize\n",
        "fig, axs = plt.subplots(1,3, figsize=(6,5))\n",
        "axs[0].imshow(sample_map, cmap='hot', vmin=0, vmax=1)\n",
        "axs[0].set_title('Sampled Map')\n",
        "axs[1].imshow(env_mask, cmap='binary')\n",
        "axs[1].set_title('Environment Mask')\n",
        "axs[2].imshow(target, cmap='hot', vmin=0, vmax=1)\n",
        "axs[2].set_title('Complete Radio Map')\n",
        "[ax.set_xticks([]) for ax in axs]\n",
        "[ax.set_yticks([]) for ax in axs]\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Output and Intermediate Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below visualizes the output of the model, with either a single channel at the final layer (i.e. the predicted output) or multiple channels at an intermediate layer (i.e. the internal representation). The user should specify the model they want to visualize and the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klmOrKSVwN58"
      },
      "outputs": [],
      "source": [
        "model_name = 'Baseline'\n",
        "model_layer = model.encoder.conv2d\n",
        "\n",
        "model = torch.load(os.path.join(model_folder, f'{model_name}.pth'), map_location=device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L_0mC4i9diW"
      },
      "outputs": [],
      "source": [
        "print('Output of Selected Layer')\n",
        "visualize_channels(x, model, model_layer, n_channels=26, nrows=5, ncols=6, figsize=(15, 15))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
